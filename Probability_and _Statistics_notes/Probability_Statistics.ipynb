{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf21c19-29ae-4c59-9fa5-75f4eec0b4f3",
   "metadata": {},
   "source": [
    "# Probability and Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812470-279d-4ce2-be20-1db7e7198d18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:red\"> 1. Basics and Definitions </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98595485-a48f-4486-8a73-5e9db77ea08c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "- Experiment \n",
    "    - Sample space \n",
    "        - Outcomes \n",
    "- Event \n",
    "    - Event space \n",
    "        - Outcomes wchich follow the rule stated in the event \n",
    "- Probability \n",
    "    - Probability space :\n",
    "        - Sample space \n",
    "        - Event space \n",
    "        - Probability function :\n",
    "            - Possibility of each and every outcome mentioned in the sample space : $P$ : $F$ $\\rightarrow$ [0,1]\n",
    "            \n",
    "---\n",
    "##### Difference between Event and experiment :\n",
    "- In probability, an event is a set of outcomes that result from an experiment, while an experiment is a process that can be repeated to produce a set of specific outcomes. The set of all possible outcomes from an experiment is called the sample space, and an event is a subset of the sample space. \n",
    "- For example, if you toss a coin, the outcomes of the experiment are \"heads\" or \"tails\", which are events associated with the experiment. Rolling a die is another example of an event, with the outcomes being (1, 2, 3, 4, 5, 6). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img_01.JPG\" alt=\"image\" width=\"400\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3a054-17d4-484b-bfc1-bae57fbc0d27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:red\"> 2. Sampling and  Repeated Trials</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fdba2-f76d-4ec0-90c6-922b67530005",
   "metadata": {},
   "source": [
    "- Applications \n",
    "    - Repeating an experiment for several times \n",
    "    - Interest : Total number of success achieved in that experiemnt.\n",
    "    - Viewed a a sampling from a large poplation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e23e51-4b92-4f8d-aaa0-b0ea53714157",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.1 Bernoulli Trials</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea67740-d133-4069-b8bd-64a6e6feae3e",
   "metadata": {},
   "source": [
    "- Trial : Trying out one experiemnt is a Trial \n",
    "- Independent Trials of an Experiment : Trying out an experiment multiple times while each and every repetiotion of an experiment is independent of each other i.e. result or outcome of any trial does not affect the outcomes of the other trials of an experiment.\n",
    "- Framework : Each trial :\n",
    "    - Success \n",
    "    - Failure\n",
    "\n",
    "<img src=\"img_02.jpg\" alt=\"image\" width=\"600\" height=\"auto\">\n",
    "\n",
    "- $p$ : Probability of success at each trial \n",
    "##### The sequence of Trials so obtained is called Bernoulli Trials with Parameter $p$\n",
    "\n",
    "- Bernoulli($p$) : single bernoulli trial  \n",
    "\n",
    "##### NOTE : \n",
    "##### we are only interested in the result of the trial, we may view this as a probability on the sample space $S$ ={$success$,$failure$} where $P$({$success$}) = $p$, but more often we will be interested in <b>multiple independent trials.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ba71d-3012-4ae1-858f-35d85a13b5bd",
   "metadata": {},
   "source": [
    "- Binomial Expansion : a and b any interger and n >= 1 \n",
    "\n",
    "#### $(a + b)^n $ = $ \\sum_{k=0}^{n} \\binom{n}{k} a^k b^{n-k} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b05a3d-8c85-4cbe-8386-119b40e27d55",
   "metadata": {},
   "source": [
    "### After performing n independent Bernoulli Trials we are interested in following questions :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3cea0e-91b9-4377-b141-395bb9a92915",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> 1. What is the probability of observing exactly k successes?</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52696c-128a-4c75-a702-d494f6f30367",
   "metadata": {},
   "source": [
    "\n",
    "- $Binomial(n,p)$ :\n",
    "    - for n = 1 : \n",
    "        - $p$({one success}) : $p$\n",
    "        - $p$({zero success}) : 1 - $p$\n",
    "    - for n > 1 :\n",
    "        - $\\omega$ = ($\\omega_1$, $\\omega_2$, ... , $\\omega_n$) : n-tuple of outcomes\n",
    "        - $S$ : set of all $\\omega$ where each $\\omega_i$ : Success or Failure \n",
    "        - $A_i$ : {i_th trial is failure}, {i_th trial is success}\n",
    "        - $P$($A_1$ $\\cap$ $A_2$ $\\cap$ ... $\\cap$  $A_n$) =  $\\prod_{i=1}^{n} P(A_i)$ \n",
    "        -  $B_k$ : Event with $k$ no. of successes among $n$  trials \n",
    "        - <br><img src=\"img_03.jpg\" alt=\"image\" width=\"600\" height=\"auto\"> <br>\n",
    "        - #### $P$( $B_k$ ) = $\\sum_{\\omega \\in B_k}  P(${${\\omega}$}$)$\n",
    "            - #### $P(${${\\omega}$}$)$ = $p^k$ $(1 - p)^{n-k}$\n",
    "            - #### $P$( $B_k$ ) = $\\sum_{\\omega \\in B_k} ( p^k$ $(1 - p)^{n-k})$\n",
    "        - #### $P$( $B_k$ ) = |$B_k$ |$( p^k$ $(1 - p)^{n-k})$\n",
    "             - But $B_k$ : event of all otcomes ($\\omega_i$) for which there are k successes and the number of ways in which $k$ successes can occur in $n$ trials is $\\binom{n}{k}$.\n",
    "             Note : All combinations for $k$ successes should be consideded.\n",
    "        - #### $P$( $B_k$ ) = $\\binom{n}{k} ( p^k$ $(1 - p)^{n-k})$\n",
    "\n",
    "---\n",
    "#### $\\sum_{k=0}^{n} \\binom{n}{k} ( p^k$ $(1 - p)^{n-k})$ = $(p + (1 - p))^n$ = 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a3c08-89de-4636-b63e-d3b31773af13",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> 2. What is the most likely number of successes?</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d925cfe-a694-4c89-8c26-4c0ae0669b80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Mode of Binomial</b>:\n",
    "- Assume 0 < $p$ < 1 and 0 <= $k$ < n and same notion for $B_k$\n",
    "- We want to determine the value of $k$ that makes $P( $B_k$)$ as large as possible; such a value is called the “mode” ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2d87f-4499-4434-a978-62fb59242491",
   "metadata": {},
   "source": [
    "- To find this value, it is instructive to compare the probability of $(k + 1)$ successes to the probability of $k$ successes : \n",
    "\n",
    " #### \n",
    "$ \\large \\frac {P(B_{k+1})}{ P(B_k)}$ = $\\large\\frac{\\binom{n}{k+1} ( p^{k+1}(1 - p)^{n-(k + 1)})}{\\binom{n}{k} ( p^k(1 - p)^{n-k})}$ = $\\large\\frac{p}{1-p} \\frac{n-k}{k+1}$\n",
    "\n",
    "- If ratio = 1 $\\Rightarrow$ {($k+1$)successes} was exactly as likely as {($k$)successes} to occur\n",
    "- If ratio > 1 $\\Rightarrow$ {($k+1$)successes} was more likely to occur as {($k$)successes} \n",
    "- If ratio < 1 $\\Rightarrow$ {($k$)successes} is more likely to occur as {($k+1$)successes} \n",
    "\n",
    "setting :\n",
    "#### $\\frac {P(B_{k+1})}{ P(B_k)}$ $\\geq$ 1 and solving for $k$ :  $k$ $\\leq$ $p(n+1)-1$\n",
    "\n",
    "\n",
    "    \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e4252-3732-4788-9994-0258ff4dc8d1",
   "metadata": {},
   "source": [
    "<img src=\"img_04.jpg\" alt=\"image\" width=\"400\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824ca20-eca3-45c1-9ab1-4899d79ed126",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> 3. How many attempts must be made before the first success is observed?</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadd4e0-bf83-4a16-80b9-5c9fa5e5afca",
   "metadata": {},
   "source": [
    "- Probability  of getting first success at first trial  = $p$\n",
    "- Probability  of getting first success at $k$th trial :\n",
    "    1. no of failures : $k-1$\n",
    "    2. $A_i$ : Event {$i$ th trial is a success}\n",
    "    3. $C_k$ : Event {the first success occurs at $k$th trial}\n",
    "Combining 2 and 3 from above :\n",
    "#### $P(C_k)$ = $P(A^{c}_1 \\cap A^{c}_2  \\cap  ...  \\cap A^{c}_{k-1}  \\cap  A_k )$\n",
    "#### $P(C_k)$ = $P(A^{c}_1)$$P(A^{c}_2)$ ... $P(A^{c}_{k-1})$$P(A_k)$\n",
    "As $P(A_i)$ = $p$ and $P(A^{c}_i)$ = $1-p$\n",
    "thus ;\n",
    "#### $P(C_k)$ = $(1-p)^{k-1}p$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac8f3a-cb4c-4e99-b11b-7b54dfdecbc4",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> 4. On average how many successes will there be?</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03033c4c-a1af-4780-8a4a-d1389ea42003",
   "metadata": {},
   "source": [
    "This is a natural question to ask but it requires a precise definition of what\n",
    "we mean by “average” in the context of probability. We shall do this in Chapter 4 and return to\n",
    "answer (d) at that point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bb17b-7dc3-434f-b778-8aa5516634a6",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.2 Poissons Approximation</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad607752-575a-4ca9-9e0d-00b5d7dae204",
   "metadata": {},
   "source": [
    "- $n$ >>>> and $p$ <<<<\n",
    "- $p \\rightarrow 0 $\n",
    "- $n \\rightarrow \\infty $\n",
    "- $np$ = $constant$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8299f95-b42b-45d9-b1a3-3bb6ae3e5e6f",
   "metadata": {},
   "source": [
    "Let $\\lambda > 0$, $k \\geq 1$, $n \\geq \\lambda$, and $p = \\frac{\\lambda}{n}$. Defining $A_k$ as\n",
    "$$A_k = \\{k \\text{ successes in } n \\text{ Bernoulli}(p) \\text{ Trials}\\},$$\n",
    "it then follows that\n",
    "$$\\lim_{{n \\to \\infty}} P(A_k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}.$$\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(A_k) &= \\binom{n}{k} \\left(\\frac{\\lambda}{n}\\right)^k \\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} \\\\\n",
    "&= \\frac{n(n-1)\\ldots(n-k+1)}{k!} \\left(\\frac{\\lambda^k}{n^k}\\right) \\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} \\\\\n",
    "&= \\frac{n^k (1 - \\frac{1}{n}) (1 - \\frac{2}{n}) \\ldots (1 - \\frac{k-1}{n})}{k!} \\left(\\frac{\\lambda^k}{n^k}\\right) \\left(1 - \\frac{\\lambda}{n}\\right)^{n-k} \\\\\n",
    "&= \\frac{\\lambda^k}{k!} \\left(1 - \\frac{1}{n}\\right) \\left(1 - \\frac{2}{n}\\right) \\ldots \\left(1 - \\frac{k-1}{n}\\right) \\left(1 - \\frac{\\lambda}{n}\\right)^{n} \\left(1 - \\frac{\\lambda}{n}\\right)^{-k} \\\\\n",
    "&= \\frac{\\lambda^k}{k!} \\left( \\prod_{j=0}^{k-1} \\left(1 - \\frac{j}{n}\\right) \\right) \\left(1 - \\frac{\\lambda}{n}\\right)^{n} \\left(1 - \\frac{\\lambda}{n}\\right)^{-k}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4756d-fdb5-46cc-a551-2a8b773be972",
   "metadata": {},
   "source": [
    "Standard limit results imply that<br>\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\left( 1 - \\frac{r}{n} \\right)^n = 1 \\text{ for all } r \\ge 1 $$ <br>\n",
    "$$ \\lim_{n \\to \\infty} \\left( 1 - \\frac{\\lambda}{n} \\right)^{-k} = 1 \\text{ for all } \\lambda \\ge 0, k \\ge 1; \\text{ and}$$ <br>\n",
    "$$ \\lim_{n \\to \\infty} \\left( 1 - \\frac{\\lambda}{n} \\right)^n = e^{-\\lambda} \\text{ for all } \\lambda \\ge 0. \n",
    "$$\n",
    "As  $P(A_k)$ is a finite product of such expressions, the result is now immediate using the properties of limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22fa4a-a1f7-4998-a937-94eafa2b9702",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 2.3 Sampling with and without Replacement </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcffa2-44b5-4c7d-97ba-624d0e74bfd4",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> 2.3.1 Hypergeometric Distribution </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a89dcb-3984-4345-a461-93a2fec85631",
   "metadata": {},
   "source": [
    "<img src=\"img_05.jpg\" alt=\"image\" width=\"600\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a9303-83a0-4c8c-a62e-425c093dc68c",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> 2.3.2 Binomial Approximation to Hypergeometric Distribution  </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85689-1b81-45f4-b407-fc8293fe2fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let $N$, $m$, and $r$ be positive integers for which $m < r < N$ and let $k$ be a positive integer between 0 and $m$.<br>\n",
    "Define \n",
    "$p$ = $\\frac{r}{N}$, $\\quad p_1$ = $\\frac{r - k}{N - k}$, $\\quad \\text{and} \\quad p_2 = \\frac{r - k}{N - m}$.\n",
    "<br>\n",
    "Letting $H$ denote the probability that a hypergeometric distribution with parameters $N$, $r$, and $m$ takes on the value $k$, the following inequalities give bounds on this probability:\n",
    "$$\n",
    "\\binom{m}{k} p_1^k (1 - p_2)^{m - k} < H \\leq \\binom{m}{k} p^k (1 - p_1)^{m - k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baad5ff-cbf7-4ede-bb64-34ea1a8dbcea",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 3. Discrete Random Variable</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ed704-3b72-4963-9069-eea4194f3c78",
   "metadata": {},
   "source": [
    "#### Random Variable : maintain a single sample space and to define functions on that space whose outputs relate them to questions under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849948d-fccc-4616-bf0f-c3f6ebb26420",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> 3.1 Random Variables as a Function </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9488a9-ecd3-4210-b925-f92bb9d8e593",
   "metadata": {},
   "source": [
    "NOTE : Rather than use the standard pre-image notation, it is more common in probability to write\n",
    "$(X = B_i)$ for the set $X^{-1}(\\{B_i\\})$ since this emphasizes that we are considering outcomes for which\n",
    "the function $X$ equals $B_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa2076-3c57-4d75-af95-ef05883af269",
   "metadata": {},
   "source": [
    "- Let $S$ be a sample space with probability $P$ and let $X : S \\rightarrow T$ be a function. Then $X$ generates a probability $Q$ on $T$ given by\n",
    "$$ Q(B) = P(X^{-1}(B)) $$\n",
    "The probability $Q$ is called the “distribution of $X$” since it describes how $X$ distributes the probability from $S$ onto $T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e82b9-f2bc-49eb-b66e-13261cc7b451",
   "metadata": {},
   "source": [
    "- The proof relies on two set-theoretic facts that we will take as given. The first is that\n",
    "$$\n",
    "X^{-1}\\left(\\bigcup_{i=1}^{\\infty} B_i\\right) = \\bigcup_{i=1}^{\\infty} X^{-1}(B_i)\n",
    "$$\n",
    "and the second is the fact that if $B_i$ and $B_j$ are disjoint, then so\n",
    "are $X^{-1}(B_i)$ and $X^{-1}(B_j)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3955a-8444-470b-9546-0e9df749299f",
   "metadata": {},
   "source": [
    "- Let $B \\subset T$. Since $P$ is known to be a probability, $0 \\le P(X^{-1}(B)) \\le 1$, and so $Q$\n",
    "maps subsets of $T$ into $[0, 1]$. Since $X$ is a function into $T$, we know $X^{-1}(T) = S$. Therefore\n",
    "$Q(T) = P(X^{-1}(T)) = P(S) = 1$ and $Q$ satisfies the first probability axiom.\n",
    "\n",
    "To show $Q$ satisfies the second axiom, suppose $B_1, B_2, \\ldots$ are a countable collection of disjoint\n",
    "subsets of $T$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q\\left(\\bigcup_{i=1}^{\\infty} B_i\\right) &= P\\left(X^{-1}\\left(\\bigcup_{i=1}^{\\infty} B_i\\right)\\right) \\\\\n",
    "&= P\\left(\\bigcup_{i=1}^{\\infty} X^{-1}(B_i)\\right) \\\\\n",
    "&= \\sum_{i=1}^{\\infty} P(X^{-1}(B_i)) \\\\\n",
    "&= \\sum_{i=1}^{\\infty} Q(B_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654db7f-45f1-4085-8166-89c371a35cc5",
   "metadata": {},
   "source": [
    "> A “discrete random variable” is a function $X : S \\rightarrow T$ where $S$ is a sample\n",
    "space equipped with a probability $P$, and $T$ is a countable (or finite) subset of the real numbers.\n",
    "From Theorem 3.1.2, $P$ generates a probability on $T$ and since it is a discrete space, the distribution may be determined by knowing the likelihood of each possible value of $X$. Because of this\n",
    "we define a function $f_X : T \\rightarrow [0, 1]$ given by\n",
    "$$ f_X(t) = P(X = t) $$\n",
    "referred to as a “probability mass function”. Then for any event $A \\subset T$ the quantity $P(X \\in A)$\n",
    "may be computed via\n",
    "$$\n",
    "P(X \\in A) = \\sum_{t \\in A} f_X(t) = \\sum_{t \\in A} P(X = t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac728459-6931-481e-84e4-d890f63d7311",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:orange\"> 3.1.1 Common Distributions </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af921ddc-ad43-4a38-9c98-7602ed54f02e",
   "metadata": {},
   "source": [
    "When studying random variables it is often more important to know <mark>how they distribute probability\n",
    "onto their range than how they actually act as functions on their domains </mark>. As such it is useful\n",
    "to have a notation that recognizes the fact that <mark>two functions may be very different in terms of\n",
    "where they map domain elements, but nevertheless have the same range and produce the same\n",
    "distribution on this range.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827ef7f-386b-4dda-a890-c9bc191adcad",
   "metadata": {},
   "source": [
    "> Let $X : S \\rightarrow T$ and $Y : S \\rightarrow T$ be discrete random variables. We say $X$ and $Y$\n",
    "have equal distribution provided $P(X = t) = P(Y = t)$ for all $t \\in T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc781e11-0b73-45c8-8281-fd11ea3b6bce",
   "metadata": {},
   "source": [
    "- ∼ means “is distributed as” or “is equal in distribution to”\n",
    "- how $X$ behaves as a function on its domain, but completely describes how $X$ distributes probability onto its range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239033bc-2740-4c6b-8691-d16ba5974171",
   "metadata": {},
   "source": [
    "#### <span style= 'color:green' >The following are common discrete distributions which we have seen arise previously in the text.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23251bdf-8356-4ab4-b9c0-c77177e8e4a2",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Uniform}(\\{1, 2, \\ldots, n\\})$: <br>\n",
    "Let $n \\geq 1$ be an integer. If $X$ is a random variable such that $$P(X = k) = \\frac{1}{n}$$ for all $1 \\leq k \\leq n$, then we say that $X$ is a uniform random variable on the set $\\{1, 2, \\ldots, n\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fbc4c-009e-4132-bd54-f4eb4fc6f810",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Bernoulli}(p)$: <br>\n",
    "Let $0 \\leq p \\leq 1$. When $X$ is a random variable such that $$P(X = 1) = p$$ and $$P(X = 0) = 1 - p$$, we say that $X$ is a Bernoulli random variable with parameter $p$. This takes the concept of a \"Bernoulli trial\" which we have previously discussed and puts it in the context of a random variable where <br>  <mark> $1$ corresponds to success and $0$ corresponds to failure.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170f0db-f7b7-4f6c-b2f2-14da12f756b3",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Binomial}(n, p)$: <br>\n",
    "Let $0 \\leq p \\leq 1$ and let $n \\geq 1$ be an integer. If $X$ is a random variable taking values in $\\{0, 1, \\ldots, n\\}$ having a probability mass function $$P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k}$$ for all $0 \\leq k \\leq n$, then $X$ is a binomial random variable with parameters $n$ and $p$. We have seen that <mark>such a quantity describes the number of successes in $n$ Bernoulli trials.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933e3b7-bc16-40ae-b339-74419358cb7f",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Geometric}(p)$: <br>\n",
    "Let $0 < p < 1$. If $X$ is a random variable with values in $\\{1, 2, 3, \\ldots\\}$ and a probability mass function $$P(X = k) = p \\cdot (1 - p)^{k-1}$$ for all $k \\geq 1$, then $X$ is a geometric random variable with parameter $p$. <mark>Such a random variable arises when determining how many Bernoulli trials must be attempted before seeing the first success.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b05dc-db7f-47e4-aa2c-df0b91d50f4d",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Negative Binomial}(r, p)$:<br>\n",
    "Let $0 < p < 1$. If $X$ is a random variable with values in $\\{r, r + 1, r + 2, \\ldots\\}$ and a probability mass function $$P(X = k) = \\binom{k-1}{r-1} p^r \\cdot (1 - p)^{k-r}$$ for all $k \\geq r$, then $X$ is a negative binomial random variable with parameters $(r, p)$. <mark>Such a random variable arises when determining how many Bernoulli trials must be attempted before seeing $r$ successes.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019b467-6031-45e3-b356-901d0b387655",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{Poisson}(\\lambda)$: <br>\n",
    "Let $\\lambda > 0$. When $X$ is a random variable with values in $\\{0, 1, 2, \\ldots\\}$ such that its probability mass function is $$P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$$ for all $k \\geq 0$, then $X$ is called a Poisson random variable with parameter $\\lambda$. <mark>We first used these distributions as approximations to a Binomial$(n, p)$ when $n$ was large and $p$ was small.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa937274-735a-41c2-b2e0-e7012a38acab",
   "metadata": {},
   "source": [
    "> $X \\sim \\text{HyperGeo}(N, r, m)$:<br>\n",
    "Let $N$, $r$, and $m$ be positive integers for which $r < N$ and $m < N$. Let $X$ be a random variable taking values in the integers between $\\min\\{m, r\\}$ and $\\max\\{0, m - (N - r)\\}$ inclusive with probability mass function $$P(X = k) = \\frac{\\binom{r}{k} \\binom{N-r}{m-k}}{\\binom{N}{m}}$$. <br>The random variable $X$ is called hypergeometric with parameters $N$, $r$, and $m$.<mark> Such quantities occur when sampling without replacement.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93579f1b-80ef-4333-b448-e0684627f076",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 3.2 Independent and Dependent Variables</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b4a02-adc7-41c5-a2f7-4f70ea992159",
   "metadata": {},
   "source": [
    "- consideration of several different random variables and an analysis of the relationships among them.\n",
    "- We have already discussed what it means for a collection of events to be independent and it is useful to extend this notion to random variables as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45d04c-a33b-4e60-a5c1-79e210c5fc65",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> 3.2.1 Independent Variables </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028f019-c9c0-466a-b492-bf6c9180e197",
   "metadata": {},
   "source": [
    ">**Independence of a Pair of Random Variables** :<br>\n",
    "Two random variables $X$ and $Y$ are independent if $(X \\in A)$ and $(Y \\in B)$ are independent for every event $A$ in the range of $X$ and every event $B$ in the range of $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe4f4c-acb7-4383-842d-ce5a2a2469c8",
   "metadata": {},
   "source": [
    "- It is common in probability to write $(X \\in A, Y \\in B)$ for the event $(X \\in A) \\cap (Y \\in B)$ \n",
    "- Even though the definition of $X : S \\to T$ and $Y : S \\to U$ being independent random variables requires that $(X \\in A)$ and $(Y \\in B)$ be independent for all events $A \\subset T$ and $B \\subset U$, for discrete random variables it is enough to verify the events $(X = t)$ and $(Y = u)$ are independent events for all $t \\in T$ and $u \\in U$ to conclude they are independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4bf94-f16e-4d3d-a72f-bf45361b5026",
   "metadata": {},
   "source": [
    "> **Mutual Independence of Random Variables**: <br>\n",
    "A finite collection of random variables $X_1, X_2, \\ldots, X_n$ is mutually independent if the sets $(X_j \\in A_j)$ are mutually independent for all events $A_j$ in the ranges of the corresponding $X_j$. An arbitrary collection of random variables $X_t$ where $t \\in I$ for some index set $I$ is mutually independent if every finite sub-collection is mutually independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a13d26-aa65-463a-ac28-e92bf5334bad",
   "metadata": {},
   "source": [
    "- For many problems it is <mark>useful to think about repeating a single experiment many times with the results of each repetition being independent from every other. </mark>\n",
    "- Though the results are assumed to be independent, the <mark>experiment itself remains the same</mark>, so the random variables produced all have the <mark>same distribution.</mark>\n",
    "- The resulting sequence of random variables $X_1, X_2, X_3, \\ldots$ is referred to as \"i.i.d.\" (standing for \"independent and identically distributed\"). When considering such sequences, we will sometimes write $X_1, X_2, X_3, \\ldots$ are i.i.d. with distribution $X$, where $X$ is a random variable that shares their common distribution.\n",
    "\n",
    "---\n",
    "\n",
    "Let $X_1, \\ldots, X_n$ be i.i.d. with a $\\text{Geometric}(p)$ distribution. What is the probability that all of these random variables are larger than some positive integer $j$?\n",
    "\n",
    "As a preliminary calculation, if $X \\sim \\text{Geometric}(p)$ and if $j \\geq 1$ is an integer we may determine $P(X > j)$.\n",
    "\n",
    "\\begin{align*}\n",
    "P(X > j) &= \\sum_{i=j+1}^{\\infty} P(X = i) \\\\\n",
    "         &= \\sum_{i=j+1}^{\\infty} p(1 - p)^{i-1} \\\\\n",
    "         &= p \\cdot (1 - p)^j \\sum_{i=0}^{\\infty} (1 - p)^i \\\\\n",
    "         &= p \\cdot (1 - p)^j \\cdot \\frac{1}{1 - (1 - p)} \\\\\n",
    "         &= (1 - p)^j\n",
    "\\end{align*}\n",
    "\n",
    "But each of $X_1, X_2, \\ldots, X_n$ have this distribution, so using the computation above, together with independence,\n",
    "\n",
    "\\begin{align*}\n",
    "P(X_1 > j, X_2 > j, \\ldots, X_n > j) &= P(X_1 > j)P(X_2 > j) \\cdots P(X_n > j) \\\\\n",
    "                                      &= (1 - p)^j \\cdot (1 - p)^j \\cdots (1 - p)^j \\\\\n",
    "                                      &= (1 - p)^{nj}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3032485-e2c6-46d8-8d0a-5bf67f357ffe",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> 3.2.2 Conditional, Joint, and Marginal Distributions </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20a9a3-c87e-4e1e-ad51-798a2cef96d8",
   "metadata": {},
   "source": [
    "- Sometimes one RV is related to other RV; knowing one should affect th probabilities associated with the values of the other. Such random variables are not independent of each other and we now introduce several ways to compute probabilities under such circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26c713-09de-4605-ad01-4b25ad69b2ad",
   "metadata": {},
   "source": [
    "- When relation between two dependent RV $X$ and $Y$ is given EXPLICITELY we use Conditional Distribution, which reflects the fact that the occurrence of an event may affect the likely values of a random variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e03583-6186-447a-9b46-6380e68b1cc9",
   "metadata": {},
   "source": [
    "> Let $X$ be a random variable on a sample space $S$ and let $A \\subset S$ be an event such that $P(A) > 0$. Then the probability $Q$ described by $Q(B) = P(X \\in B | A)$ is called the \"conditional distribution\" of $X$ given the event $A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19c2de-a92c-45cf-b759-cbb2f1e13fe7",
   "metadata": {},
   "source": [
    "- As with any discrete random variable, <mark>the distribution is completely determined by the probabilities associated with each possible value the random variable may assume.</mark>\n",
    "- This means the <mark>conditional distribution may be considered known provided the values of $P(X = a | A)$ are known for every $a \\in \\text{Range}(X)$.</mark><br>\n",
    "\n",
    "---\n",
    "\n",
    "NOTE : Though this definition allows for $A$ to be any sort of event, in this section we will mainly consider examples where $A$ describes the outcome of some random variable. So a notation like $P(X | Y = b)$ will be the conditional distribution of the random variable $X$ given that the variable $Y$ is known to have the value $b$.\n",
    "\n",
    "##### In many cases random variables are dependent in such a way that the distribution of one variable is known in terms of the values taken on by another\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7790f4-a2ef-48f6-93a3-34a3080f2c32",
   "metadata": {},
   "source": [
    "- Frequently random variables may be dependent in some way that is not so explicitly described. A more general method of expressing the dependence of two (or more) variables is to present the probabilities associated with all combinations of possible values for every variable. This is known as their joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c31cda-da03-4eb1-9a4b-4e148994eadd",
   "metadata": {},
   "source": [
    "> If $X$ and $Y$ are discrete random variables, the \"joint distribution\" of $X$ and $Y$ is the probability $Q$ on pairs of values in the ranges of $X$ and $Y$ defined by $Q((a, b)) = P(X = a, Y = b)$.<br><br>\n",
    "The definition may be expanded to a finite collection of discrete random variables $X_1, X_2, \\ldots, X_n$ for which the joint distribution of all $n$ variables is the probability defined by $Q((a_1, a_2, \\ldots, a_n)) = P(X_1 = a_1, X_2 = a_2, \\ldots, X_n = a_n)$.\n",
    "> ---\n",
    "> In the above definition, as discussed before for any event $D$,$$ Q(D) = \\sum_{(a_1,a_2,\\ldots,a_n) \\in D} Q((a_1, a_2, \\ldots , a_n))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697f5fb-efd0-4269-9fd0-ade8e3b9073e",
   "metadata": {},
   "source": [
    "NOTE : For a pair of random variables with few possible outcomes, it is common to describe the joint\n",
    "distribution using a chart for which the columns correspond to possible X values, the rows to\n",
    "possible Y values, and for which the entries of the chart are probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1c298-d955-4ad7-8456-88fc2dbb7977",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> 3.3 Functions of Random Variables</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66095b9-2933-4805-8708-460f0f477de4",
   "metadata": {},
   "source": [
    "-  Many circumstances where we want to consider functions applied to random variables as inputs of functions.\n",
    "- If $X : S \\to T$ is a random variable and if $f : T \\to \\mathbb{R}$ is a function, then the quantity $f(X)$ makes sense as a composition of functions $f \\circ X : S \\to \\mathbb{R}$. In fact, since $f(X)$ is defined on the sample space $S$, this new composition is itself a random variable.\n",
    "- The same reasoning holds for functions of more than one variable. If $X_1, X_2, \\ldots, X_n$ are random variables, then $f(X_1, X_2, \\ldots, X_n)$ is a random variable provided $f$ is defined for the values the $X_j$ variables produce. Below we illustrate how to calculate the distribution of $f(X_1, X_2, \\ldots, X_n)$ in terms of the joint distribution of the $X_j$ input variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636110b1-e7e8-4780-bdab-6cb71fe1704e",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> 3.3.1 Distribution of $f(X)$ and $f(X_1, X_2, \\ldots, X_n)$ </span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c2724-a466-46a8-9bdb-79d767239eff",
   "metadata": {},
   "source": [
    "- The distribution of $f(X)$ involves the probability of events such as $(f(X) = a)$ for values of $a$ that the function may produce. The key to calculating this probability is that these events may be rewritten in terms of the input values of $X$ instead of the output values of $f(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07433c-d9e4-4f3d-9e33-e72cd242f959",
   "metadata": {},
   "source": [
    "-  addition is one of the most common examples of applying functions to random quantities. \n",
    "- Suppose $X$ and $Y$ are random variables taking values in $\\{0, 1, 2, \\ldots\\}$ and suppose $Z = X + Y$.\n",
    " How could $P(Z = n)$ be calculated?\n",
    "- Since both $X$ and $Y$ are non-negative and since $Z = X + Y$, the value of $Z$ must be at least as large as either $X$ or $Y$ individually. \n",
    "- If $Z = n$, then $X$ could take on any value $j \\in \\{0, 1, \\ldots, n\\}$, but once that value is determined, the value of $Y$ is compelled to be $n - j$ to give the appropriate sum. \n",
    "- In other words, the event $(Z = n)$ partitions into the following union:\n",
    "$$\n",
    "(Z = n) = \\bigcup_{j=0}^{n} (X = j, Y = n - j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcbbe48-adb8-47ad-8976-5fddbc442dc2",
   "metadata": {},
   "source": [
    "When $X$ and $Y$ are independent, this means :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Z = n) &= P\\left(\\bigcup_{j=0}^{n} (X = j, Y = n - j)\\right) \\\\\n",
    "         &= \\sum_{j=0}^{n} P(X = j, Y = n - j) \\\\\n",
    "         &= \\sum_{j=0}^{n} P(X = j) \\cdot P(Y = n - j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "<br>\n",
    "- Such a computation is usually referred to as a “convolution” which will be addressed more generally later in the text. It occurs regularly when determining the distribution of sums of independent random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfd9c5-c2e6-425d-97a8-c097f9f2d3c8",
   "metadata": {},
   "source": [
    "##### Consider the below example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b18d38-eaf6-4f87-911c-f1e4a5697d9c",
   "metadata": {},
   "source": [
    "**Example 3.3.4.** Let $X \\sim \\text{Poisson}(\\lambda_1)$ and $Y \\sim \\text{Poisson}(\\lambda_2)$ be independent random variables.\n",
    "\n",
    "(a) Let $Z = X + Y$. Find the distribution of $Z$.\n",
    "\n",
    "(b) Find the conditional distribution of $X \\mid Z$.\n",
    "\n",
    "Now, <br>\n",
    "$$\n",
    "P(X = x, Y = y) = P(X = x) \\cdot P(Y = y) = e^{-\\lambda_1} \\frac{\\lambda_1^x}{x!} \\cdot e^{-\\lambda_2} \\frac{\\lambda_2^y}{y!} \\quad \\text{for} \\quad x, y \\in \\{0, 1, 2, \\ldots \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a88b9-e935-4f5e-a7d8-708e691761ba",
   "metadata": {},
   "source": [
    "(a) As computed above the distribution of $Z$ is given by the convolution. For any $n = 0, 1, 2, \\ldots$ we have <br><br>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Z = n) &= P(X + Y = n) \\\\\n",
    "         &= \\sum_{j=0}^n P(X = j) \\cdot P(Y = n - j) \\\\\n",
    "         &= \\sum_{j=0}^n e^{-\\lambda_1} \\frac{\\lambda_1^j}{j!} \\cdot e^{-\\lambda_2} \\frac{\\lambda_2^{n-j}}{(n - j)!} \\\\\n",
    "         &= e^{-(\\lambda_1 + \\lambda_2)} \\sum_{j=0}^n \\frac{\\lambda_1^j}{j!} \\frac{\\lambda_2^{n-j}}{(n - j)!} \\\\\n",
    "         &= e^{-(\\lambda_1 + \\lambda_2)} \\frac{1}{n!} \\sum_{j=0}^n \\frac{n!}{j!(n - j)!} \\lambda_1^j \\lambda_2^{n-j} \\\\\n",
    "         &= e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^n}{n!}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100e766-c8e6-443c-85b2-b48ca91f3767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
